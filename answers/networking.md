# Topic: NETWORKING

## How TCP open a connection? What does it need to open a connection?

TCP (Transmission Control Protocol) is one of the main protocols in the Internet Protocol (IP) suite, and it provides reliable, connection-oriented communication between devices on a network. To open a connection, TCP uses a process known as the three-way handshake:

1. **SYN (Synchronize)**
   - The process begins with the client sending a TCP packet with the SYN flag set to the server. This packet contains an initial sequence number (ISN) generated by the client.
   - The client is essentially saying, "I want to establish a connection with you, and here is my initial sequence number."

2. **SYN-ACK (Synchronize - Acknowledge)**
   - Upon receiving the SYN packet, if the server is willing to establish a connection, it responds with a packet that has both the SYN and ACK flags set.
   - The server also generates its own initial sequence number (ISN) and includes it in the packet.
   - The server is essentially saying, "I acknowledge your request to connect, and I also want to establish a connection with you. Here's my initial sequence number."

3. **ACK (Acknowledge)**
   - Finally, the client acknowledges the server's response by sending a packet with the ACK flag set.
   - This packet also contains an acknowledgment number, which is the server's initial sequence number incremented by 1.
   - The client is essentially saying, "I acknowledge your acknowledgment. Let's start communicating."

At this point, the connection is established, and both parties can start sending data to each other reliably.

In addition to the three-way handshake, there are several other aspects that TCP needs to open a connection:

1. **IP Addresses**: Each device on a network has a unique IP address. This is crucial for routing packets between devices.

2. **Port Numbers**: Along with the IP address, a port number is used to identify specific services or processes on a device. For example, HTTP typically uses port 80, while HTTPS uses port 443.

3. **Socket**: A socket is a combination of an IP address and a port number. Together, they uniquely identify a specific endpoint on a network.

4. **Buffering and Flow Control**: TCP provides mechanisms for managing the flow of data between sender and receiver, ensuring that data isn't sent too fast for the recipient to handle.

5. **Error Detection and Correction**: TCP includes mechanisms for detecting and correcting errors in data transmission to ensure the integrity of the communication.

These elements work together to enable TCP to establish reliable connections between devices on a network, allowing for the transmission of data in a way that ensures it arrives intact and in the correct order.

## Why there are 3 way handshakes but not 2 way?

The three-way handshake is used in TCP for several important reasons, which a two-way handshake wouldn't adequately address:

1. **Initiator and Responder Identification**: In a two-way handshake, if the server simply acknowledged the client's request to connect, there would be no way for the server to communicate its own intent to establish a connection. The three-way handshake allows both parties to express their willingness to establish a connection.

2. **Synchronization of Sequence Numbers**: The initial sequence numbers (ISNs) exchanged during the handshake are crucial for maintaining the order of data segments in the communication. In a two-way handshake, if the server were to respond with just an acknowledgment, it would have no way to communicate its own ISN to the client. This could lead to problems with sequence number synchronization.

3. **Prevents Half-Open Connections**: A two-way handshake could potentially lead to a situation known as a "half-open connection." This occurs when one party believes a connection is established, but the other does not. The third step of the three-way handshake helps prevent this scenario by confirming the connection on both sides.

4. **Security**: The three-way handshake introduces an additional step that helps protect against certain types of attacks, like SYN flood attacks. These attacks involve sending a large number of SYN packets without completing the handshake, potentially overwhelming a server's resources. The three-way handshake ensures that both parties are invested in the connection.

5. **Completeness of Communication**: Each step in the three-way handshake serves a specific purpose: initiation, acknowledgment, and confirmation. These steps work together to establish a full, mutual agreement to initiate a connection.

Overall, the three-way handshake is a well-designed mechanism that helps ensure reliable and secure communication in TCP. It's a fundamental part of how TCP operates, and it provides benefits that a two-way handshake wouldn't be able to offer.

## What is syn, ack mean?

Syn means synchronizaton. SYN packet is sent when the sender tries to connect to the receiver.

Ack means acknowledgment. ACK packet is sent by the receiver to the sender to help the sender know that the SYN packet that it sent before reached the destination successfully.

## Why they have to send 2 "random" sequence numbers? The purpose of this sequence number?

Sequence numbers are used to keep track of the order of data. The initial sequence number needs to be chosen randomly so the data in one flow will not be conflicted with other data bytes in other flows which are transmitted in the same TCP connection. Because sequence number is 32-bit integer so the maximum value of a sequence number is 2^32 - 1. So if the amount of data is greater than 4GB, at least one sequence number value will be reused.

## What if the 3rd handshake fail? How the server can detect it and what does it do in this case?

If the third handshake (the ACK from the client) were to fail, it would indicate that the client did not receive the acknowledgment from the server. This could happen due to a variety of reasons, such as network congestion, a misconfigured firewall, or a temporary network issue.

Here's what would typically happen if the third handshake fails:

1. **Timeout and Retransmission**:
   - After a certain period of time without receiving the expected ACK, the client will assume that the acknowledgment was lost in transit and will retransmit the SYN packet.
   - The server, upon receiving this retransmitted SYN packet, will again respond with a SYN-ACK.

2. **Connection Establishment Attempt (Retry)**:
   - The client will continue attempting to establish the connection by sending the SYN packet.
   - The server will respond with SYN-ACK as long as it is still willing to establish the connection.

3. **Connection Termination** (Eventually):
   - If the server decides to give up on establishing the connection (perhaps due to a timeout on its end), it will not respond to subsequent SYN packets from the client. This would effectively signal the client that the connection could not be established.

It's important to note that TCP is designed to be robust and to handle scenarios where packets are lost or delayed. This is why it includes mechanisms for retransmission of data and acknowledgment of received data.

In practice, if the third handshake consistently fails, it might indicate a more serious issue, such as a network misconfiguration or a problem with one of the systems involved. Troubleshooting such issues would involve examining network configurations, firewall settings, and potentially conducting network diagnostics to identify and rectify the underlying problem.

## How TCP handles the connection?

TCP (Transmission Control Protocol) handles connections in a reliable and connection-oriented manner. Here's a brief overview of how TCP manages connections:

1. **Connection Establishment**:
   - As discussed earlier, TCP uses a three-way handshake to establish a connection between a client and a server. This involves the exchange of SYN (synchronize) and ACK (acknowledge) packets to confirm both parties' intent to establish a connection.

2. **Data Transmission**:
   - Once a connection is established, data can be transmitted in both directions (from the client to the server and vice versa).
   - TCP segments the data into manageable chunks called segments and adds sequence numbers to them.

3. **Acknowledgment and Flow Control**:
   - After receiving a segment, the receiver sends back an acknowledgment (ACK) to confirm receipt of the data. The ACK includes the next expected sequence number.
   - TCP also implements flow control mechanisms to prevent overwhelming the receiver with data. The receiver advertises its receive window size, indicating how much data it can currently accept.

4. **Retransmission**:
   - If a segment is not acknowledged within a certain timeout period, the sender assumes it was lost in transit and retransmits it. This ensures reliable data delivery.

5. **Out-of-Order Data Handling**:
   - TCP is designed to handle out-of-order delivery of segments. Each segment has a sequence number, allowing the receiver to correctly order the data upon receipt.

6. **Congestion Control**:
   - TCP monitors network conditions to avoid congestion. It adapts its transmission rate based on the available bandwidth and network congestion signals.

7. **Connection Termination**:
   - When the communication is complete, a four-way handshake is used to terminate the connection. This involves sending FIN (finish) and ACK packets to signal the intent to close the connection.

8. **TIME_WAIT State**:
   - After a connection is closed, TCP enters a TIME_WAIT state to ensure that any delayed packets related to the closed connection are handled properly. This prevents confusion with new connections using the same port numbers.

9. **Connection State Management**:
   - Throughout the connection, TCP maintains a state for each connection. This includes information about sequence numbers, window sizes, and other parameters.

10. **Error Detection and Correction**:
    - TCP uses checksums to verify the integrity of data segments. If a segment is received with a checksum error, it is discarded, and the receiver may request retransmission.

By incorporating these features, TCP ensures reliable, ordered, and flow-controlled data delivery over IP networks, making it a fundamental protocol for many applications on the internet.

## What happens if some bits are wrong due to connection errors? How to detect them and fix them?

TCP uses a few mechanisms to detect and handle errors caused by incorrect or corrupted bits during data transmission:

1. **Checksums**:
   - TCP employs a checksum algorithm to detect errors in the received data. When a segment is received, the receiver calculates the checksum based on the received bits and compares it to the checksum value included in the segment's header.
   - If the calculated checksum does not match the received checksum, the segment is considered corrupt, and it is discarded. The receiver may request the sender to retransmit the corrupted segment.

2. **Retransmission**:
   - If a segment is not acknowledged within a certain timeout period, or if it's determined to be corrupt due to a checksum mismatch, the sender will retransmit the segment. This ensures that data is reliably delivered, even in the presence of errors.

3. **Sequence Numbers**:
   - TCP uses sequence numbers to ensure that data segments are delivered in the correct order. If a segment arrives out of order, the receiver will hold it until the missing segments are received, preventing incorrect data from being delivered.

4. **Selective Acknowledgment (SACK)**:
   - In some cases, TCP implementations may use a SACK option to allow the receiver to specify which segments were received successfully and which were not. This provides more granular feedback to the sender, enabling it to retransmit only the necessary segments.

5. **Flow Control**:
   - TCP's flow control mechanisms prevent the sender from overwhelming the receiver with data. The receiver advertises a window size, indicating how much data it can currently accept. This helps prevent congestion and reduces the likelihood of errors.

6. **Error Detection and Correction Codes (ECC)**:
   - In some cases, particularly in high-reliability environments, additional error detection and correction codes may be applied at the hardware level. These codes can detect and correct certain types of errors.

7. **Automatic Repeat reQuest (ARQ)**:
   - Some lower-level protocols (e.g., at the link layer) may implement ARQ techniques. ARQ involves automatic retransmission of packets when errors are detected.

It's important to note that while TCP provides reliable data transmission, it's not foolproof. There are rare situations where errors may occur despite these mechanisms. For instance, if a segment is corrupted in a way that the checksum still matches (a very unlikely event), the error might not be detected. In such cases, higher-level protocols or applications may need to implement their own error-checking mechanisms.

## How the timeout is handled? what if the timeout is expired?

The timeout in TCP is a crucial aspect of ensuring reliable communication. It's used to handle situations where an acknowledgment (ACK) is not received within a reasonable amount of time. Here's how it works:

1. **Sending Data**:
   - When a sender transmits a TCP segment, it starts a timer associated with that segment. This timer is set based on the expected round-trip time (RTT) for the network.

2. **Expecting an Acknowledgment**:
   - The sender expects to receive an acknowledgment (ACK) from the receiver within the timeout period. The ACK confirms that the receiver has successfully received the data.

3. **Timeout Expiry**:
   - If the timer expires before the acknowledgment is received, it indicates that the ACK was lost in transit or the receiver did not receive the segment.
  
4. **Retransmission**:
   - Upon timeout, the sender assumes the segment was lost and retransmits it. This is a crucial part of TCP's reliability. It ensures that data is delivered even if an acknowledgment is not received.

5. **Adaptive Retransmission**:
   - TCP uses various algorithms to dynamically adjust the timeout value based on network conditions. For example, if the sender consistently receives acknowledgments quickly, it may decrease the timeout to expedite retransmissions. Conversely, if acknowledgments are consistently delayed, it may increase the timeout to allow for longer network delays.

6. **Backoff Strategies**:
   - TCP also employs backoff strategies to prevent congestion in case of continuous retransmissions. For example, the sender might double the timeout value after each unsuccessful attempt.

7. **Fast Retransmit and Fast Recovery**:
   - TCP includes mechanisms for detecting and recovering from packet loss more quickly than waiting for a timeout. This is known as Fast Retransmit and Fast Recovery. If a sender receives duplicate acknowledgments, it assumes a segment was lost and retransmits it without waiting for a timeout.

8. **Congestion Control**:
   - TCP also considers timeouts as a potential sign of network congestion. If timeouts occur too frequently, it may adjust its transmission rate to alleviate congestion.

If a timeout occurs and retransmissions do not result in successful delivery, the sender and receiver may continue to attempt communication. However, if the connection remains unresponsive for an extended period, the application or higher-level protocol may need to handle the situation (e.g., by closing the connection or alerting the user).

## What will happen if some "packet" is missing on the way?

If a packet goes missing in a TCP connection, several steps are taken to ensure reliable data delivery:

1. **Timeout and Retransmission**:
   - The sender expects to receive an acknowledgment (ACK) from the receiver within a certain timeout period. If the timer expires before the acknowledgment is received, the sender assumes the packet was lost in transit and retransmits it.

2. **Selective Acknowledgment (SACK)**:
   - In some cases, TCP implementations may use a SACK option to allow the receiver to specify which segments were received successfully and which were not. This provides more granular feedback to the sender, enabling it to retransmit only the necessary segments.

3. **Reassembly and In-Order Delivery**:
   - TCP uses sequence numbers to ensure that data segments are delivered in the correct order. If a packet arrives out of order, the receiver will hold it until the missing packets are received, ensuring that data is delivered in the correct sequence.

4. **Flow Control and Congestion Control**:
   - TCP's flow control mechanisms prevent the sender from overwhelming the receiver with data. The receiver advertises a window size, indicating how much data it can currently accept. This helps prevent congestion and reduces the likelihood of missing packets.

5. **Detecting Missing Packets through Acknowledgments**:
   - When the receiver acknowledges received data, it includes the next expected sequence number. If there are gaps in the received sequence numbers, it indicates that some packets are missing.

6. **Retransmission Timer Adjustments**:
   - If retransmissions are occurring frequently, TCP's retransmission timer may be adjusted dynamically to adapt to network conditions. For example, if packets are consistently taking longer to be acknowledged, the timer may be increased to allow for longer round-trip times.

7. **Fast Retransmit and Fast Recovery**:
   - If the sender receives duplicate acknowledgments (indicating that a packet may be lost), it may trigger a fast retransmit to resend the missing packet without waiting for a timeout.

8. **Upper Layer Handling**:
   - Higher-level protocols or applications may implement their own mechanisms for dealing with missing data. For example, a file transfer protocol may include its own error-checking and retransmission mechanisms.

In summary, TCP is designed to handle missing packets through a combination of retransmissions, reassembly of out-of-order data, and dynamic adjustments to timers and transmission rates. These mechanisms work together to ensure reliable data delivery, even in the presence of network issues or packet loss.

## How to detect the appropriate number of packets to send (speed of sending packet)?

Like above.

## How TCP close the connection?

TCP uses a four-way handshake to gracefully close a connection. Here's how the process works:

1. **Initiation of Connection Closure**:
   - The process of closing a TCP connection begins when one party (either the client or the server) decides it no longer needs the connection and wants to terminate it.

2. **Sending a FIN (Finish) Segment**:
   - The party initiating the closure sends a TCP segment with the FIN flag set to the other party. This segment indicates that it wants to close the connection for sending data.

3. **Acknowledgment of FIN**:
   - Upon receiving the FIN segment, the receiving party acknowledges it with an ACK segment. This acknowledges the receipt of the FIN and indicates that it is prepared to close its end of the connection for sending data.

4. **Time-Wait State**:
   - The party that sent the initial FIN enters a "TIME_WAIT" state. This state ensures that any delayed packets related to the closed connection are handled properly. It also helps prevent confusion with new connections using the same port numbers.

5. **Waiting for Final ACK**:
   - During the TIME_WAIT state, the party waits for a final acknowledgment (ACK) from the other party to confirm the closure of the connection.

6. **Second FIN and ACK**:
   - After a brief period in the TIME_WAIT state, the party that received the initial FIN may also decide to close its end of the connection. It sends a FIN segment to the other party.

7. **Acknowledgment of Second FIN**:
   - The receiving party sends an ACK segment in response to the second FIN, confirming the receipt and indicating its readiness to close its end of the connection.

8. **Connection Closed**:
   - At this point, the connection is considered closed, and both parties are free to reuse the resources associated with that connection.

The TIME_WAIT state helps prevent potential issues with delayed or duplicate segments related to the closed connection. It's an important part of TCP's reliability and ensures that the connection is closed safely.

It's worth noting that while this four-way handshake is the standard process for closing a TCP connection, there are cases where connections can be abruptly terminated due to network issues or application-level decisions. In such cases, the TIME_WAIT state might not be observed, and the connection may close more abruptly.

## What if the internet is dropped in the middle of the connection? Or in case one peer is crash?

When unexpected events occur, such as the loss of internet connectivity or a peer crashing during a TCP connection, the behavior and recovery process depend on the specific circumstances:

1. **Loss of Internet Connectivity**:

   - If the internet connection drops abruptly, both peers will eventually realize that they have lost contact with each other.
   
   - The TCP connection will enter a timeout state, waiting for acknowledgment packets that will never arrive.
   
   - After a period of time (determined by the operating system's TCP/IP stack), the connection will be considered "dead" and the resources associated with it will be released.

   - Any unsent or unacknowledged data will be lost. When the connection is reestablished, a new connection will need to be established.

2. **Peer Crash**:

   - If one of the peers crashes unexpectedly, the other peer will eventually realize that it has lost contact with the crashed peer.
   
   - The TCP connection will enter a timeout state, waiting for acknowledgment packets that will never arrive.
   
   - Similar to the scenario of a lost internet connection, after a period of time, the connection will be considered "dead" and the resources will be released.
   
   - Any unsent or unacknowledged data will be lost.

   - When the crashed peer recovers, it will no longer have any knowledge of the previous connection. If it needs to communicate with the other peer, it will need to establish a new connection.

In both cases, the key point is that TCP is a reliable protocol, but it relies on the underlying network to provide reliable transport. If the network experiences unexpected failures, TCP will do its best to recover, but there are limits to what it can handle.

Additionally, if the application requires higher levels of reliability or fault tolerance, it may need to implement its own mechanisms for dealing with such scenarios, such as application-level acknowledgments, data synchronization, or reconnection strategies.

## How long you can keep a TCP connection alive?

The duration that a TCP connection can remain open, also known as its "idle timeout" or "inactivity timeout," is determined by various factors, including network configurations, operating system settings, and specific applications or services involved. 

In practice, idle timeouts for TCP connections can range from a few minutes to several hours, depending on the specific environment. Here are some considerations:

1. **Operating System Settings**:
   - Operating systems have default values for TCP idle timeouts. For example, in some systems, the default idle timeout might be around 2 hours. These values can often be configured or adjusted by system administrators.

2. **Network Devices and Firewalls**:
   - Network devices, such as routers and firewalls, can also have their own idle timeout settings. These devices might terminate idle connections after a certain period to conserve resources.

3. **Application-Specific Configurations**:
   - Some applications or services have their own configurable idle timeout settings. For example, web servers, databases, and other network services may have parameters that control how long a connection can remain idle.

4. **Load Balancers and Proxies**:
   - If a connection passes through load balancers or proxies, these intermediate devices may have their own timeout settings that can impact the duration a connection remains open.

5. **Keep-Alive Mechanisms**:
   - Many applications implement their own keep-alive mechanisms to prevent idle connections from being terminated. This involves periodically sending small packets or messages to keep the connection active.

6. **Specific Protocols**:
   - Some protocols, such as HTTP, may have their own mechanisms for controlling how long a connection can remain idle. For example, HTTP/1.1 supports persistent connections that can remain open for multiple requests.

7. **Provider and Infrastructure Policies**:
   - Cloud providers, ISPs, and other infrastructure services may have their own policies and configurations that impact how long TCP connections can remain open.

8. **Security Considerations**:
   - Longer idle timeouts can potentially increase the risk of certain types of attacks, such as idle scan attacks or resource exhaustion attacks. Therefore, security policies may influence the duration of idle connections.

Ultimately, the specific duration a TCP connection can remain open will depend on the configurations and policies in place in the particular network environment. It's important for administrators and developers to understand and configure these settings based on the requirements of their applications and services.

## What are the differences between TCP and UDP? And in which case we use which?

TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) are two of the core protocols in the Internet Protocol (IP) suite. They serve different purposes and have distinct characteristics. Here are the key differences between TCP and UDP:

1. **Connection-Oriented vs. Connectionless**:
   - **TCP**: Connection-oriented protocol. It establishes a connection before data transmission and ensures reliable delivery through mechanisms like acknowledgments, retransmissions, and flow control.
   - **UDP**: Connectionless protocol. It does not establish a connection and does not provide reliability or guarantees of delivery. It simply sends data without any verification.

2. **Reliability**:
   - **TCP**: Provides reliable delivery of data. It guarantees that data will be received in the correct order without errors.
   - **UDP**: Does not guarantee reliability. Data may be lost, arrive out of order, or contain errors.

3. **Flow Control**:
   - **TCP**: Implements flow control mechanisms to prevent the sender from overwhelming the receiver with data.
   - **UDP**: Does not implement flow control. The sender can send data at its own rate, regardless of the receiver's ability to handle it.

4. **Acknowledgment and Retransmission**:
   - **TCP**: Uses acknowledgments (ACKs) and retransmissions to ensure data delivery. If a segment is not acknowledged within a certain time, it will be retransmitted.
   - **UDP**: Does not use acknowledgments or retransmissions. It's up to the application to handle any reliability requirements.

5. **Header Size**:
   - **TCP**: Has a larger header size due to the additional control information required for reliable communication.
   - **UDP**: Has a smaller header size since it doesn't include the overhead for acknowledgments and sequencing.

6. **Latency**:
   - **TCP**: Can introduce higher latency due to the establishment of connections, acknowledgments, and retransmissions.
   - **UDP**: Typically has lower latency because it doesn't have the overhead of connection setup and error recovery.

7. **Use Cases**:
   - **TCP** is commonly used for applications that require reliable, ordered, and error-checked delivery of data, such as web browsing, email, file transfer, and remote desktop connections.
   
   - **UDP** is used for applications where speed and efficiency are more critical than guaranteed delivery, such as real-time multimedia streaming, online gaming, DNS (Domain Name System) queries, and VoIP (Voice over Internet Protocol).

8. **Example Protocols**:
   - **TCP**: HTTP, HTTPS, FTP, SMTP, Telnet, SSH
   - **UDP**: DNS, DHCP, SNMP, TFTP, VoIP, online gaming protocols (e.g., UDP-based protocols for games like UDP-based protocols for games like DNS, DHCP, SNMP, TFTP, VoIP, online gaming protocols (e.g., UDP-based protocols for games like UDP-based protocols for games like Fortnite, League of Legends)

In summary, TCP provides reliable, connection-oriented communication with features like reliability, flow control, and error correction. UDP, on the other hand, is faster and more lightweight, making it suitable for applications where speed is crucial, even if it means sacrificing some reliability. The choice between TCP and UDP depends on the specific requirements of the application.

## How Ping command works? What is TTL? How does TTL will be changed?

Ping command sends a packet to the destination server, waits for the response then calculates round trip time.

Time-to-live is a mechanism that prevents a packet to be forward around the internet forever. Like the name, it is the number of times or hops, that packet can be forward before it is discarded by a router. Each time a router receives a packet, it reduces TTL of that packet by 1. If TTL reaches zero, the router will discard this packet then send an ICMP message back to the sender. The recommended default value of TTL is 64.

## How HTTP works?

HTTP (Hypertext Transfer Protocol) is a protocol used for communication between a client (such as a web browser) and a web server. It governs the request and response process for fetching and transmitting web resources like HTML pages, images, stylesheets, and more. Here's a basic overview of how HTTP works:

1. **Client-Server Interaction**:
   - HTTP operates on a client-server model, where a client (such as a web browser) sends requests to a web server, which processes the request and returns a response.

2. **Request**:
   - The process begins with the client sending an HTTP request to a specific URL (Uniform Resource Locator) or URI (Uniform Resource Identifier). The request is composed of:
     - A method (e.g., GET, POST, PUT, DELETE) that defines the action to be performed.
     - The path to the requested resource.
     - Headers containing additional information like user agent, accepted content types, etc.
     - Optionally, a request body for methods like POST or PUT, which can contain data to be sent to the server.

3. **Server Processing**:
   - The web server receives the request and processes it based on the provided method, path, and headers. It locates the requested resource on the server.

4. **Resource Retrieval**:
   - If the request is successful, the server fetches the requested resource (e.g., an HTML file, image, or other content) from its storage.

5. **Response**:
   - The server constructs an HTTP response, which includes:
     - A status line indicating the result of the request (e.g., HTTP version, status code, and a brief description).
     - Headers providing additional information about the response (e.g., content type, date, server type).
     - The actual content (body) of the response, which contains the requested resource.

6. **Transmission**:
   - The server transmits the response back to the client over the established TCP connection. This is usually done over port 80 for HTTP or port 443 for HTTPS.

7. **Client Processing**:
   - The client receives the response and processes it based on the provided status code and content. The browser may render HTML content, display images, execute JavaScript, and perform other actions.

8. **Closure or Persistent Connection**:
   - In HTTP/1.1 and later versions, connections can be kept open (persistent connections) for multiple requests, improving performance by avoiding the overhead of repeatedly establishing and tearing down connections. This is controlled by the "Connection" header.

9. **Optional Additional Requests**:
   - The client may issue additional requests for resources referenced within the HTML page (e.g., images, stylesheets, scripts). These additional requests follow the same process.

10. **Session Management (Optional)**:
    - For applications that require user sessions (e.g., user login, shopping carts), mechanisms like cookies or session IDs are used to maintain state between requests.

HTTP is a stateless protocol, which means each request-response cycle is independent and does not inherently retain information about previous interactions. To handle stateful interactions, techniques like cookies or server-side sessions are used.

Note: HTTPS (HTTP Secure) is a secure version of HTTP that uses encryption (SSL/TLS) to secure the communication between the client and server. It operates on port 443 instead of the standard port 80.

## Why did people say that HTTP is stateless? The reason they make it stateless?

HTTP (Hypertext Transfer Protocol) is considered stateless because each request-response cycle is independent and does not inherently retain information about previous interactions. In other words, the server does not remember past requests from a client.

There are several reasons why HTTP was designed to be stateless:

1. **Simplicity**:
   - Statelessness simplifies the protocol. It reduces the complexity of handling and managing state information on both the client and server sides.

2. **Scalability**:
   - Stateless interactions make it easier to scale web applications. Because each request does not rely on past interactions, servers can process requests from different clients in parallel without needing to maintain context for each individual client.

3. **Flexibility**:
   - Stateless design allows for flexibility in handling requests from various clients and devices. A server does not need to maintain specific information about each client, which makes it more adaptable to a wide range of scenarios.

4. **Fault Tolerance**:
   - Stateless interactions make it easier to recover from failures. If a server or client encounters an error, it can simply retry the request without having to worry about the state of previous interactions.

5. **Caching**:
   - Stateless design is conducive to caching. Responses to requests can be easily cached by intermediaries (e.g., proxy servers, CDN edge nodes) to improve performance and reduce the load on origin servers.

6. **Loose Coupling**:
   - Stateless interactions promote loose coupling between clients and servers. Clients do not need to maintain a continuous connection with the server, which allows for more flexibility in how clients and servers are implemented.

However, while HTTP itself is stateless, many web applications require some form of state to maintain user sessions, track user authentication, and handle activities like shopping carts. To address this, techniques like cookies, session IDs, and application-level session management are used to create and manage stateful interactions on top of the stateless HTTP protocol.

By keeping HTTP stateless at its core, it remains a versatile and adaptable protocol that can serve a wide range of web applications and scenarios, from simple static content delivery to complex interactive web applications.

## Can we make a persistent HTTP connection? pros and cons of this way?
Yes, it's possible to establish a persistent HTTP connection, which allows multiple requests and responses to be sent over the same connection. This is commonly referred to as "HTTP Keep-Alive" or "HTTP Persistent Connection."

**Pros of Persistent HTTP Connections:**

1. **Reduced Latency**: Persistent connections eliminate the need to repeatedly establish and tear down connections, reducing the overhead associated with connection setup.

2. **Improved Performance**: By reusing existing connections, subsequent requests can be sent and received more quickly, leading to faster overall page loading times.

3. **Resource Efficiency**: Fewer resources are used on both the client and server sides because fewer connections need to be established and maintained.

4. **Reduced Network Congestion**: Fewer connections mean less congestion on the network, which can lead to a smoother experience for users.

5. **Better Use of Server Resources**: The server can handle a higher number of requests per connection, which can be especially beneficial for servers handling a large number of clients.

6. **Better Suited for Modern Web Applications**: Modern web applications often make multiple requests to load resources (e.g., HTML, CSS, JavaScript, images). Persistent connections align well with this behavior.

**Cons of Persistent HTTP Connections:**

1. **Potential for Resource Exhaustion**: If not managed properly, a large number of persistent connections can consume server resources, such as memory and processing power.

2. **Complexity of Connection Management**: Implementing and managing persistent connections can add complexity to both the client and server codebases.

3. **Potential for Connection Pools**: In high-traffic scenarios, managing a pool of connections may be necessary, which adds additional complexity.

4. **Timeouts and Cleanup**: Servers need to implement timeouts and cleanup mechanisms to close idle connections and free up resources.

5. **Load Balancing Challenges**: Some load balancers may not handle persistent connections well, potentially leading to uneven distribution of requests.

6. **Not Supported by All Servers or Clients**: While widely supported, there may be cases where older or specialized servers/clients do not fully support persistent connections.

Overall, while there are some challenges associated with managing persistent connections, the benefits, especially for modern web applications with many resource requests, often outweigh the drawbacks. Properly implemented, persistent connections can significantly improve the performance and efficiency of web applications.

## Why HTTP require cookie each time we send the request?

HTTP is a stateless protocol, meaning that each request-response cycle is independent and does not inherently retain information about previous interactions. This statelessness is one of the reasons why HTTP is scalable and efficient, as it allows servers to process requests from different clients in parallel without needing to maintain context for each individual client.

However, there are scenarios where it's necessary to maintain state information between multiple requests. This is where cookies come into play. Cookies are small pieces of data that are sent by the server to the client's browser and are stored on the client's side. The browser then includes the cookie in subsequent requests to the same server.

Here are the reasons why cookies are used and why they need to be sent with each request:

1. **Maintaining Session State**:
   - Cookies are commonly used to maintain session state. For example, in a web application, when a user logs in, a session is created on the server. A unique session ID is stored in a cookie, allowing the server to associate subsequent requests with the correct session.

2. **Personalization and User Preferences**:
   - Cookies can be used to remember user preferences or settings. For example, a website might remember a user's preferred language or display options.

3. **Tracking and Analytics**:
   - Cookies can be used for tracking user behavior on a website. This can be used for analytics, personalization, and targeting of content or advertisements.

4. **Shopping Carts and E-Commerce**:
   - Cookies are often used in e-commerce websites to maintain a user's shopping cart. The contents of the cart are stored in a cookie, allowing the user to continue shopping without losing their selections.

5. **Authentication and Authorization**:
   - Cookies are used to maintain authentication information. After a user logs in, a cookie with an authentication token is sent with each subsequent request to verify the user's identity.

6. **Load Balancing and Session Affinity**:
   - In environments with multiple servers, cookies can be used to maintain session affinity, ensuring that subsequent requests from a client are directed to the same server.

7. **Cross-Site Request Forgery (CSRF) Protection**:
   - Cookies can be used to mitigate CSRF attacks by including a unique token in each request. This token is checked on the server to verify the authenticity of the request.

8. **Cross-Origin Resource Sharing (CORS)**:
   - Cookies are used in conjunction with CORS policies to control which domains are allowed to access resources on a web page.

In summary, cookies are a fundamental mechanism for maintaining stateful interactions in a stateless protocol like HTTP. They allow web applications to remember user-specific information and provide a personalized experience. By including cookies in each request, the server can associate the request with the correct user or session.

## Can someone use your cookie and log in your Facebook account? How to migrate this?

If someone gains unauthorized access to your cookies, they can potentially use them to impersonate you on websites that rely on cookies for authentication, including social media platforms like Facebook. This is known as a session hijacking or cookie hijacking attack.

To protect against this, it's crucial to take measures to secure your cookies and mitigate the risk of unauthorized access. Here are some steps you can take:

1. **Use HTTPS**:
   - Always use websites that use HTTPS (SSL/TLS encryption). This encrypts the data in transit, making it much more difficult for attackers to intercept and use your cookies.

2. **Avoid Public Wi-Fi for Sensitive Activities**:
   - Avoid logging in to sensitive accounts like social media or online banking when connected to unsecured public Wi-Fi networks. If you need to access sensitive accounts while on public Wi-Fi, consider using a Virtual Private Network (VPN) for added security.

3. **Log Out of Accounts**:
   - Always log out of your accounts when you're finished using them, especially on shared or public computers. This prevents others from gaining access to your session.

4. **Use Private Browsing or Incognito Mode**:
   - Browsers have a private browsing or incognito mode that doesn't save cookies, history, or other browsing data. This can be useful when using public computers.

5. **Regularly Clear Cookies**:
   - Periodically clear your browser's cookies and cache. This can help prevent the buildup of potentially sensitive data.

6. **Enable Two-Factor Authentication (2FA)**:
   - Enable 2FA for your accounts whenever possible. Even if someone gains access to your cookies, they would still need an additional authentication factor to log in.

7. **Monitor Account Activity**:
   - Regularly review your account activity for any suspicious or unauthorized logins. Most platforms, including Facebook, provide tools to review recent login activity.

8. **Change Passwords Periodically**:
   - Change your passwords regularly. This can help prevent unauthorized access even if someone gains access to your cookies.

9. **Avoid Clicking on Suspicious Links**:
   - Be cautious when clicking on links, especially in emails or messages from unknown sources. These links may lead to phishing sites that attempt to steal your login credentials.

10. **Use a Cookie Management Extension**:
    - Consider using browser extensions or add-ons that provide enhanced cookie management and security features.

If you suspect that your cookies or account have been compromised, take immediate action:

- Change your password.
- Log out of all active sessions (if the service provides this feature).
- Review and update your account security settings.
- Monitor your account for any unusual activity.

By following these steps, you can significantly reduce the risk of unauthorized access to your accounts through cookie hijacking.

## What is HTTP session? How does authentication work in HTTP? What is JWT?

**HTTP Session:**
An HTTP session is a way to maintain stateful interactions between a client (e.g., web browser) and a server over a series of HTTP requests. It allows the server to remember information about a user's interactions with a website or web application across multiple requests. Sessions are typically used to store user-specific data, such as authentication status, shopping cart contents, and other application-specific information.

Here's how an HTTP session works:

1. **Session Creation**: When a user first visits a website, a unique session identifier is generated by the server. This identifier is typically stored in a cookie on the client's browser.

2. **Session Tracking**: On subsequent requests, the client sends the session identifier (usually in a cookie) back to the server. The server uses this identifier to retrieve the user's session data.

3. **Data Storage**: The server can store data related to the session, such as user-specific information or application state, in memory or in a database.

4. **Session Termination**: Sessions can be terminated explicitly (e.g., when a user logs out) or automatically after a period of inactivity (controlled by session timeout settings).

**Authentication in HTTP:**
Authentication in HTTP involves verifying the identity of a user or system making a request to a server. It's a crucial part of securing web applications and services. There are several methods of authentication, including:

1. **Basic Authentication**:
   - Basic Authentication involves sending a username and password with each request. However, this method is considered less secure because credentials are sent in plaintext, making them susceptible to interception.

2. **Digest Authentication**:
   - Digest Authentication is an improvement over Basic Authentication. It involves sending a hashed version of the password with each request, providing a higher level of security.

3. **OAuth**:
   - OAuth is an open standard for access delegation, commonly used for authorization rather than authentication. It allows a third-party application to access resources on a user's behalf without exposing their credentials.

4. **OAuth 2.0**:
   - OAuth 2.0 is a widely used authorization framework that allows applications to gain limited access to a user's resources without exposing their credentials.

5. **JWT (JSON Web Tokens)**:
   - JWT is a compact, URL-safe token format that can be used for securely transmitting information between parties. It is often used for authentication and information exchange. JWTs are digitally signed, ensuring their integrity.

**JWT (JSON Web Tokens):**
JSON Web Tokens (JWT) are a standard format for representing claims between two parties in a compact and self-contained way. They are commonly used for authentication and information exchange in web applications and APIs. JWTs consist of three parts:

1. **Header**: Contains information about how the token is encoded and signed (e.g., algorithm used for signing).
2. **Payload**: Contains claims (statements about an entity) encoded in JSON format. Claims can be user information, permissions, expiration time, and more.
3. **Signature**: Created by combining the header, payload, and a secret key, which ensures the integrity of the token.

JWTs are often used in scenarios where stateless authentication is required. They allow servers to verify the authenticity of a user without the need to maintain a server-side session. When a user logs in, a JWT is generated and sent to the client. The client includes the JWT in the headers of subsequent requests. The server verifies the JWT's signature and uses the information in the payload to authenticate the user.

Overall, JWTs provide a secure and efficient way to handle authentication and information exchange in modern web applications and APIs.

## Which type of "data" HTTP can help us to get or push? (binary file? image? text file? video file? music file?)

HTTP can help us get or push all kinds of files like text, image, binary, video, music...

## REST/RESTful?

REST stands for representational state transfer. It's a set of constraints that describes how an API should work. It includes four main principles:

- Client-Sever: There is always a client and a server. Client sends request to server and server reply with response.
- Stateless: Server handles client request independently. Client's request must contain enough information that server requires.
- Uniform interface: For example, a resource is identified in URI, HTTP response always comes with status and body.
- Cacheability: Client can cache response.

An API that follows REST constraints is called RESTful.

## AJAX technique?

AJAX uses XMLHttpRequest to communicate with server. The first "A" stands for asynchronous, which means using AJAX, we can communicate with server without reloading the page.

## How HTTPS work?

HTTPS uses SSL - Secure Socket Layer to protect the communication by encryption. Before exchanging data, an SSL connection must be established first, then data will be encrypted then transferred safely.

## Learn about some useful headers

To be defined.

## When you type "google.com" into your browser, that will happen when you type enter till everything is displayed on your screen?

The first thing that happens is DNS lookup. The browser will get the corresponding IP address with the domain name from its cache, from the computer cache, from the local host file. If there is no data in these places, then the browser will send a request to DNS server to obtain the IP address. ([More](https://serverfault.com/questions/208172/how-does-my-browser-know-where-to-get-data-from-a-server))

The second thing is browser will detect if HTTPS is required or not in case we don't type the full url. Browser can get this information from its history data, from a preload list. Otherwise, it will send an HTTP request. (When server receives the request, server can do appropriate action to force user uses HTTPS if it wants.)

The third thing is TCP connection establishment. The TCP connection can be established by a process called 3-way handshake. If HTTPS is used, after TCP connection is established, the SSL connection establishment process will start.

Fourthly, after having the connection, client will send data to the server and server will send a response back to client through this connection.

The next thing is browser will render the data from response based on the content type. If there is no content type, the broswer will read the body of response to know which kind of data it is. If the content type is html, broswer will parse it then get resources like images, js files, css files if necessary.

## DNS lookup (in case you already access google.com before and also in case you do not know the IP of google.com)

DNS information is cached in browser and operating system. If there is no corresponding data in both of these places, DNS request will be sent to the DNS resolver. The resolver will make necessary request to Top Level Domain server, to Domain Name server to get the IP address of the requested host, then return to user.

## Which protocol DNS use and why?

DNS primarily uses UDP as transport protocol because it's fast and DNS data usually is small and fits an UDP packet.

TCP is used when the data is greater than 512 bytes, which can not be fit on a single UDP packet.

## The other of place to look up DNS.

To be defined.

## TCP or UDP will be used in this case? why?

To be defined.

## How to know "google.com" require HTTP or HTTPS? how browser can know and redirect from HTTP to HTTPS?

There are at least two ways that make the web we are accessing change from using HTTP to HTTPS.

The first way is the server will redirect to HTTPS when the HTTP request comes.

The second way is using HSTS, HTTP strict transport security. When a browser sends the first HTTP request to server, server will add a header into the response indicates that the next request will require using HTTPS. Browser will automatically use HTTPS next time based on that information. In addition, browsers like Chrome have a HSTS preload list, they will automatically use HTTPS for all of the sites in that list.

## After you get the HTML content for "google.com" how to get the \*.js and image files?

Chain requests.

## When getting \*.js or image files do why use another TCP connection or use the same one as in the get HTML content? How DNS lookup work in this case?

To be defined.

## After your browser display "google.com" fully, is there any connection open?

It depends on if the connection is keep alive or not.

## Caching can apply to which steps? How caching applied?

To be defined.

## What is the connection pool? It's advantages and disadvantages? How to implement connection pool in your programing language?

Connection pool is a cache of database connections so connection can be reused when future request comes. Opening a connection is costly, so after a connection is opened, it is placed into connection pool so it can be reused later.

## What is socket?

Socket is an endpoint for a program to send and receive data across the network. Socket allows communication between processes on the same machine or different machines.

## Why do we need socket? Why socket is a "file" in linux?

Not only socket, everything is treated as files in Linux operating system. It provides a common and universal interface for things to communicate in this OS.

## What is src port when you create a connection to a "server"?

To be defined.

## How one server can handle multiple connections to the same port?

As below.

## What is the maximum number of connections a server can handle? (if it has unlimited resource) (in case of the same client and in case of multiple clients)

TCP has 4 fields to identify the uniqueness of a socket: source IP, source port, destination IP, destination port.
Theoretically, if server listens on a port, a client can open a maximum of 2^16 connection. that's is the maximum number of ports a client can open. For multiple clients, the number can be multiplied by 2^32, it is the possible number of source IP addresses. So the maximum number of connections is 2^48.

But in practice, the maximum number of connections depends on the kernel configuration. Because each socket needs a file descriptor, the number of sockets is limited by the maximum number of file descriptors that the machine allows. In addition, a file descriptor needs a certain amount of memory so it can also be limited by the number of available memory.

## When you open multiple tabs on your chrome, how OS knows which packet (both sending and receiving) correspond to which tab? (how about in se you open many tabs to the same page "for eg: google.com")

Because each connection has its own socket that is used to exchange data.

## What are the maximum numbers of connection your machine can connect to "google.com" (if you have unlimited resource)

It is the maximum number of ports my machine can open, which is 2^16.

## Can two processes listen to the same port on your machine? Why? How?

Yes, and it is useful for multicast purpose. Socket needs to be specified as a reused socket before binding to do so.

## What is buffer? why we always need buffer when working with "file"?

Buffer is a segment of memory that is reserved to store necessary data being processed. Buffer helps reduce the processing time.

## What is unix socket? When to use it?

Unix socket or IPC socket is a data communication endpoint that allows processes in a same machine to communicate with others.

## What is TCP proxy? reverse proxy? and VPN?

A TCP proxy server is a server that acts as an intermediary between and client and server. Its goal is to hide clients on the internet. The server only knows that request came from the proxy server and doesn't know about the actual clients.

A reverse proxy, on the other hand, protect server from client. It hides server from client. from the client point of view, client sends data to the reverse proxy and receives data from it too.

VPN allows devices on a public network to be connected as they are in a private network.

## How your router at your home works?

Router connects devices within a network by forwarding packets between them. Packets might be sent between devices, or from a device to the internet and vice versa. It maintains an ARP (address resolution protocol) table to know where to forward a packet to.

## Inside LAN network, it uses IP or MAC address? Why?

Inside a LAN network, it uses MAC address cause we don't need a layer 3 protocol to communicate between devices that are in the same local network.

## How does it know which packet comes from (or arrive at) which machine?

Router will use the ARP table to know the MAC address of the device which this packet is sent to.

## What is the difference between Hub and Switch inside LAN?

Hub is operated on Physical layer and supports only broadcast transmission.

Switch is operated on Data link layer and supports unicast, multicast, and broadcast transmission.s

## How src IP/PORT and dst IP/PORT change on the way to the server?

To be defined.

## How load balancer works?

Load-balancer distributes load to servers. It can do it randomly, uses an algorithm like round-robin, or distribute requests based on the current load of each server.

## When we send a packet to a load balancer how does it forward to the desired server? (Does it keep any data on its memory?)

As above (No).

## When the server wants to send data back to the client, does the connection need to go through the load balancer?

It depends on whether the server and the load balancer are in the same private network and share the same IP address.

## What is different between reverse proxy and load balancer?

Reserve proxy acts as an intermediary without any load distributing operation.

## Can load balancer be a bottleneck? (Because it is the end point of too many requests) (bottleneck about RAM or CPU or Network?)

Yes. And it also can be a single point of failure.

Certainly! Let's dive into more details:

1. **VPN (Virtual Private Network)**: It creates a secure connection between two remote offices or devices over the public internet. VPNs use encryption to protect data during transmission.

2. **Cloud Computing**: It involves delivering various services (such as storage, databases, servers, networking, analytics, etc.) over the internet. Cloud computing provides scalability, flexibility, and cost-effectiveness.

3. **Internetworking**: It refers to connecting different networks together to create a larger network. Routers and gateways play a crucial role in internetworking.

4. **OSI Model Layers**:
   - **Application Layer**: Provides network services directly to end-users (e.g., HTTP, SMTP).
   - **Presentation Layer**: Handles data translation, encryption, and compression.
   - **Session Layer**: Manages communication sessions between devices.

5. **HTTPS (Hypertext Transfer Protocol Secure)**:
   - It ensures secure communication over the internet by encrypting data using SSL/TLS.
   - Typically operates on port 443.

6. **Services at the Application Layer**:
   - **Mail Services**: Email protocols like SMTP, IMAP, and POP3.
   - **Directory Services**: LDAP (Lightweight Directory Access Protocol).
   - **File Transfer**: FTP (File Transfer Protocol).
   - **Access Management**: Authentication and authorization.
   - **Network Virtual Terminal**: Remote login services (e.g., Telnet).

7. **Header and Trailer Addition**:
   - Headers are added at layers 6 (Presentation), 5 (Session), 4 (Transport), and 3 (Network) of the OSI model.
   - The Data Link layer adds a trailer for error detection.

Feel free to ask if you'd like further elaboration! 
